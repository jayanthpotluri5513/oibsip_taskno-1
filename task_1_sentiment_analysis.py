# -*- coding: utf-8 -*-
"""Task-1 Sentiment analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ETFU0MojjTfkKRlfnVK2wyXNVfUoUdmh

# SENTIMENT ANALYSIS ON TWITTER DATASET

# Reading the dataset
"""

## Importing all the essential modules
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
import nltk

df=pd.read_csv("drive/MyDrive/Twitter_Data.csv")
df.head()

"""# Data Cleaning"""

df=df.head(50000)

df.shape

df.isnull().sum()

df.shape

df=df.dropna()

df.isnull().sum()

df.head()

"""# Data Visualization"""

import matplotlib.pyplot as plt
custom_colors = ["#FF5733", "#33FFA8", "#337CFF"]
import seaborn as sns
fig = plt.figure(figsize=(7,5))
sns.countplot(x="category",data=df, palette=custom_colors)

fig = plt.figure(figsize=(10,10))
colors = ("yellowgreen", "gold", "red")
wp = {'linewidth':2, 'edgecolor':"black"}
tags = df['category'].value_counts()
explode = (0.1,0.1,0.1)
tags.plot(kind='pie', autopct='%1.1f%%', shadow=True, colors = colors,
         startangle=90, wedgeprops = wp, explode = explode, label='')
plt.title('Distribution of sentiments')

from wordcloud import WordCloud
wc = WordCloud(width = 700, height = 500, min_font_size = 10, background_color = 'black')
positive_wc = wc.generate(df[df['category'] == 1.0]['clean_text'].str.cat(sep = " "))
neutral_wc = wc.generate(df[df['category'] == 0.0]['clean_text'].str.cat(sep = " "))
negative_wc = wc.generate(df[df['category'] == -1.0]['clean_text'].str.cat(sep = " "))
plt.figure(figsize = (12, 12))
plt.title('wordcloud for positive review')
plt.imshow(positive_wc)

plt.figure(figsize = (10, 10))
plt.title('wordcloud for negative review')
plt.imshow(negative_wc)

"""# Cleaning text data in the dataset"""

from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
import string
nltk.download('punkt')
nltk.download('wordnet')

# Data preprocessing for removing unnessential components from the text
def preprocess(text):
  text=text.lower()
  tokens=word_tokenize(text)
  text = re.sub(r'[^a-zA-Z\s]', '', text)
  text=  re.sub('[0-9]+', '', text)
  stop_words=set(stopwords.words('english'))
  tokens=[word for word in tokens if word not in stop_words]

  lemmatizer = WordNetLemmatizer()

  tokens= [lemmatizer.lemmatize(word) for word in tokens]

  tokens=[word for word in tokens if word.isalnum()]
  preprocessed_text =' '.join(tokens)

  return preprocessed_text

def preprocess_column(df, input_col, output_col):
    df[output_col] = df[input_col].apply(preprocess)
    return df

df=preprocess_column(df,'clean_text','fresh_text')

df.head()

"""# Converting Text into Vectors

**Using countVectorizer and TfidfVectorizer**
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

df.drop(columns=['clean_text'],inplace=True)

df.head()

df.shape

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB,BernoulliNB
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

bow_v=CountVectorizer(max_df=0.70, min_df=2, stop_words='english')
bow=bow_v.fit_transform(df['fresh_text'])

"""# Splitting the dataset"""

X=df['fresh_text'].values.astype('U')
y=df['category'].values.astype('U')

X

y

X_train, X_test, y_train, y_test = train_test_split(bow, y, test_size=0.25, random_state=42)

vectorizer = TfidfVectorizer(ngram_range=(1,2))

# Fit and transform the training data
X_train_tfidf = vectorizer.fit_transform(X_train)

# Transform the testing data
X_test_tfidf = vectorizer.transform(X_test)

"""# Building different Machine Learning Models"""

def train_multiclass_models(X_train, y_train, X_test, y_test):
    models = {
        'Logistic Regression': LogisticRegression(),
        'Multinomial Naive Bayes': MultinomialNB(),
        'Bernoulli Naive Bayes': BernoulliNB(),
        'Random Forest': RandomForestClassifier(),
    }

    # Initialize results dictionary
    results = {}

    # Train and predict for each model
    for model_name, model in models.items():
        print(f"Training {model_name}...")
        # Train the model
        model.fit(X_train, y_train)

        # Predict on the test set
        y_pred = model.predict(X_test)

        # Calculate accuracy
        accuracy = accuracy_score(y_test, y_pred)

        # Store accuracy for the model
        results[model_name] = accuracy

    return results

results = train_multiclass_models(X_train,y_train,X_test, y_test)

"""# Final Results"""

results

plt.figure(figsize=(10, 6))
plt.barh(range(len(results)), list(results.values()), align='center', color='skyblue')
plt.yticks(range(len(results)), list(results.keys()))
plt.xlabel('Accuracy')
plt.title('Comparison of Multiclass Classification Models')
plt.gca().invert_yaxis()  # Invert y-axis to have the highest accuracy at the top
plt.show()

"""**As we know that Machine Learning tend to perform poorly on the text data due to fact of variable length sentences and they don't capture semantic relationship between words thus we use Deep Learning models like LSTM etc.**

# Using Deep Neural Network Architecture
"""

X = df['clean_text'].values.astype('U')
y = df['category'].values.astype('U')
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y)

labels = pd.get_dummies(df.category)
labels.columns = ["negative", "neutral", "positive"]

labels.head()

"""# Preparing data for training LSTM model"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
X_sequences = tokenizer.texts_to_sequences(X)
X_padded = pad_sequences(X_sequences)
X_train_seq, X_test_seq, y_train, y_test = train_test_split(X_padded, labels, test_size=0.2, stratify=labels)

"""# Building Model"""

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=X_padded.shape[1]),
    tf.keras.layers.LSTM(100),
    tf.keras.layers.Dense(3, activation='softmax')])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history=model.fit(X_train_seq, y_train, epochs=5, batch_size=64, validation_split=0.2)

"""# Results"""

loss, accuracy = model.evaluate(X_test_seq, y_test)
print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss vs Validation Loss')
plt.legend()
plt.show()

"""# Model Architecture"""

from tensorflow.keras.utils import plot_model
plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)
img = plt.imread('model_architecture.png')
plt.figure(figsize=(12, 12))
plt.imshow(img)
plt.show()

